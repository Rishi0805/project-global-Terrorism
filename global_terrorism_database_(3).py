# -*- coding: utf-8 -*-
"""Global terrorism database (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18CgshZA_UwXbndE_eIrvLb7YcagEAjdA
"""

import pandas as pd
import numpy as np

file_path = 'globalterrorismdb_2021Jan-June_1222dist.xlsx'

#datasource = https://www.start.umd.edu/gtd/contact/download?t=38f4a8088ba511ef95ab0e5194896103

# Read the excel file
df = pd.read_excel(file_path)

# Now the data can be worked with the DataFrame 'df' containing the data from the excel file
print(df.head())  # Print the first few rows of the DataFrame as a check

!pip install openpyxl

df.tail(20)

# Set the 'max_columns' option to None to print all columns
pd.set_option('display.max_columns', None)

# Now, when you print the DataFrame,  display all columns without truncation
print(df)

# Step 1: Get insights about the dataset
print("Dataset Info:")
print(df.info())

print("\nSummary Statistics:")
print(df.describe())

# Step 2: Deal with null values
null_percentages = df.isnull().mean() * 100

print("\nPercentage of null values in each column:")
print(null_percentages)

# Create a dataframe with the sorted percentages
null_df = pd.DataFrame({'Column': null_percentages.index, 'Null Percentage': null_percentages.values})

# Display all rows of the dataframe
pd.set_option('display.max_rows', None)
print(null_df)

# 1. Remove columns with extremely high null percentages (>95%) or redundant information
cols_to_drop = [
    'resolution', 'approxdate', 'alternative', 'alternative_txt',
    'attacktype2', 'attacktype2_txt', 'attacktype3', 'attacktype3_txt',
    'targtype2', 'targtype2_txt', 'targsubtype2', 'targsubtype2_txt', 'corp2', 'target2', 'natlty2', 'natlty2_txt',
    'targtype3', 'targtype3_txt', 'targsubtype3', 'targsubtype3_txt', 'corp3', 'target3', 'natlty3', 'natlty3_txt',
    'gsubname', 'gname2', 'gsubname2', 'gname3', 'gsubname3', 'guncertain2', 'guncertain3',
    'claim2', 'claimmode2', 'claimmode2_txt', 'claim3', 'claimmode3', 'claimmode3_txt', 'compclaim',
    'weaptype2', 'weaptype2_txt', 'weapsubtype2', 'weapsubtype2_txt',
    'weaptype3', 'weaptype3_txt', 'weapsubtype3', 'weapsubtype3_txt',
    'weaptype4', 'weaptype4_txt', 'weapsubtype4', 'weapsubtype4_txt',
    'nhostkidus', 'nhours', 'ndays', 'kidhijcountry', 'ransom', 'ransomamt', 'ransomamtus', 'ransompaid', 'ransompaidus', 'ransomnote',
    'scite2', 'scite3', 'related'
]

# Calculate null percentages for the specified columns
null_percentages = df[cols_to_drop].isnull().mean() * 100

df = df.drop(columns=cols_to_drop)
print(f"Shape after dropping high-null columns: {df.shape}")

# Create a DataFrame with the results
results_df = pd.DataFrame({
    'Column': null_percentages.index,
    'Null Percentage': null_percentages.values
})

results_df

df.info()

df.isnull().sum()

# 1. Handle missing values in 'city' column
df['city'].fillna('Unknown', inplace=True)

# 2. Handle missing values in 'latitude' and 'longitude'
# For geographical data, we might want to drop rows with missing coordinates
df.dropna(subset=['latitude', 'longitude'], inplace=True)

# Since it's a numerical variable from 1 to 5, we can use the median to fill missing values
df['specificity'].fillna(df['specificity'].median(), inplace=True)

df.isnull().sum()

# Calculate the percentage of missing values for each column
missing_percent = (df.isnull().sum() / len(df)) * 100

# Display missing percentages in descending order
missing_percent = missing_percent.sort_values(ascending=False)
print(missing_percent)

# Comment: This step calculates and displays the percentage of missing values for each column.
# Columns with a high percentage of missing values may need to be dropped or imputed.

# Define a threshold for dropping columns (e.g., drop columns with more than 50% missing values)
threshold = 50

# Drop columns where the percentage of missing values exceeds the threshold
columns_to_drop = missing_percent[missing_percent > threshold].index
df = df.drop(columns=columns_to_drop)

print(f"Columns dropped: {list(columns_to_drop)}")

# Comment: This step removes columns with excessive missing values based on a defined threshold.
# Dropping such columns ensures that they do not negatively impact model performance.

df.tail()

df.isnull().sum()

# Step 1: Identify columns with missing values
missing_columns = df.columns[df.isnull().sum() > 0]
print("Columns with missing values and their counts:")
print(df[missing_columns].isnull().sum())

# Step 2: Impute missing values based on column type and context

# Fill numerical columns with mean or median based on skewness
numerical_columns = ['multiple', 'targsubtype1', 'nkill', 'nkillus', 'nkillter',
                     'nwound', 'nwoundus', 'nwoundte']

for column in numerical_columns:
    if df[column].isnull().sum() > 0:
        # Use mean if the column is normally distributed; otherwise, use median
        if abs(df[column].skew()) < 1:  # Skewness threshold for normality
            df[column].fillna(df[column].mean(), inplace=True)
            print(f"Filled missing values in '{column}' with mean.")
        else:
            df[column].fillna(df[column].median(), inplace=True)
            print(f"Filled missing values in '{column}' with median.")

# Fill categorical columns with mode (most frequent value)
categorical_columns = ['targsubtype1_txt', 'natlty1', 'natlty1_txt',
                       'weapsubtype1', 'weapsubtype1_txt']

for column in categorical_columns:
    if df[column].isnull().sum() > 0:
        df[column].fillna(df[column].mode()[0], inplace=True)
        print(f"Filled missing values in '{column}' with mode (most frequent value).")

# Step 3: Verify that all missing values have been handled
print("\nMissing values after imputation:")
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder, StandardScaler

# Step 1: Separate categorical and numerical columns
categorical_columns = df.select_dtypes(include=['object']).columns
numerical_columns = df.select_dtypes(include=['float64', 'int64']).columns

print("Categorical Columns:", list(categorical_columns))
print("Numerical Columns:", list(numerical_columns))

# Step 2: Encode categorical variables
# Use Label Encoding for simplicity (One-Hot Encoding can also be used if needed)
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le  # Save the encoder for inverse transformation if needed
    print(f"Encoded '{column}' using Label Encoding.")

df

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.compose import ColumnTransformer

# Define columns to scale
continuous_cols = ['latitude', 'longitude', 'nkill', 'nkillus',
                   'nkillter', 'nwound', 'nwoundus', 'nwoundte']
discrete_cols = ['specificity', 'multiple']

# Define the transformer
scaler = ColumnTransformer(
    transformers=[
        ('standard_scaler', StandardScaler(), continuous_cols),
        ('minmax_scaler', MinMaxScaler(), discrete_cols)
    ],
    remainder='passthrough'  # Keep other columns unchanged
)

# Apply scaling
scaled_data = scaler.fit_transform(df)

# Convert back to DataFrame for easier inspection
scaled_df = pd.DataFrame(scaled_data, columns=df.columns)
print("\nScaled DataFrame:")
print(scaled_df.head())

import pandas as pd
from sklearn.preprocessing import StandardScaler

# Step 2: Identify columns to scale
columns_to_scale = ['latitude', 'longitude', 'specificity', 'nkill', 'nkillus',
                    'nkillter', 'nwound', 'nwoundus', 'nwoundte']

# Step 3: Apply StandardScaler only to selected columns
scaler = StandardScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

# Step 4: Leave other columns unchanged
print("\nScaled DataFrame (First 5 Rows):")
print(df.head())



from sklearn.preprocessing import StandardScaler

# Step 1: Identify numerical columns that need scaling
columns_to_scale = ['latitude', 'longitude', 'specificity', 'nkill', 'nkillus',
                    'nkillter', 'nwound', 'nwoundus', 'nwoundte']

# Step 2: Scale only these numerical columns
scaler = StandardScaler()
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])
print("Scaled only relevant numerical columns.")

# Step 3: Verify scaled values
print("\nScaled Columns (First 5 Rows):")
print(df[columns_to_scale].head())

df

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go

# Example: Bar chart for 'country_txt'
country_counts = df['country_txt'].value_counts().reset_index()
country_counts.columns = ['Country', 'Frequency']  # Rename columns for clarity

fig = px.bar(
    country_counts,
    x='Country',
    y='Frequency',
    labels={'Country': 'Country', 'Frequency': 'Frequency'},
    title='Frequency of Attacks by Country',
)
fig.update_layout(xaxis=dict(tickangle=45))  # Rotate x-axis labels for better readability
fig.show()

# List of categorical columns
categorical_columns = ['country_txt', 'region_txt', 'attacktype1_txt',
                       'targtype1_txt', 'weaptype1_txt']

# Generate bar charts for each categorical column
for column in categorical_columns:
    value_counts = df[column].value_counts().reset_index()
    value_counts.columns = [column, 'Frequency']  # Rename columns

    fig = px.bar(
        value_counts,
        x=column,
        y='Frequency',
        title=f'Frequency Distribution of {column}',
        labels={column: column, 'Frequency': 'Frequency'},
    )
    fig.update_layout(xaxis=dict(tickangle=45))  # Rotate x-axis labels
    fig.show()

# List of numerical columns
numerical_columns = ['nkill', 'nwound', 'latitude', 'longitude']

# Histogram for numerical columns
for column in numerical_columns:
    fig = px.histogram(
        df,
        x=column,
        nbins=50,
        title=f'Distribution of {column}',
        labels={column: column},
    )
    fig.show()

# Box plot for numerical columns
for column in numerical_columns:
    fig = px.box(
        df,
        y=column,
        title=f'Box Plot of {column}',
        labels={column: column},
    )
    fig.show()

# List of binary columns
binary_columns = ['success', 'suicide', 'INT_LOG', 'INT_IDEO', 'INT_MISC', 'INT_ANY']

# Pie chart for binary columns
for column in binary_columns:
    fig = px.pie(
        df,
        names=column,
        title=f'Proportion of {column}',
        hole=0.4,  # Donut chart style
    )
    fig.show()

# Scatter plot: Latitude vs Longitude (Geographical Distribution)
fig = px.scatter(
    df,
    x='longitude',
    y='latitude',
    color='region_txt',  # Color by region
    title='Geographical Distribution of Attacks',
    labels={'longitude': 'Longitude', 'latitude': 'Latitude'},
)
fig.show()

df.isnull().sum()

# Impute missing values in 'nperps' and 'nperpcap' with their respective mean
df['nperps'] = df['nperps'].fillna(df['nperps'].mean())
df['nperpcap'] = df['nperpcap'].fillna(df['nperpcap'].mean())

# Verify that there are no more missing values in these columns
print("Missing values after imputation:")
print(df[['nperps', 'nperpcap']].isnull().sum())

df.isnull().sum()

# Step 1: Compute the correlation matrix for the entire DataFrame
correlation_matrix = df.corr()

# Step 2: Display the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Step 3: Identify highly correlated features (absolute correlation > 0.9)
threshold = 0.9
high_corr_pairs = []

# Iterate through the correlation matrix to find pairs of highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

# Display highly correlated pairs
print("\nHighly Correlated Feature Pairs (|correlation| > 0.9):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]} with correlation = {pair[2]:.2f}")

# Step 4: Drop one feature from each highly correlated pair (optional)
features_to_drop = [pair[1] for pair in high_corr_pairs]  # Drop the second feature in each pair
df_reduced = df.drop(columns=features_to_drop)

print("\nDropped Features:")
print(features_to_drop)

# Step 5: Verify the reduced DataFrame
print("\nReduced DataFrame Columns:")
print(df_reduced.columns)

df['extended'].unique()

# Drop the 'dbsource' column from the DataFrame
df = df.drop(columns=['dbsource','iyear'])

# Verify that the column has been removed
print("Columns after removing 'dbsource':")
print(df.columns)

# Step 1: Compute the correlation matrix for the entire DataFrame
correlation_matrix = df.corr()

# Step 2: Display the correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Step 3: Identify highly correlated features (absolute correlation > 0.9)
threshold = 0.9
high_corr_pairs = []

# Iterate through the correlation matrix to find pairs of highly correlated features
for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

# Display highly correlated pairs
print("\nHighly Correlated Feature Pairs (|correlation| > 0.9):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]} with correlation = {pair[2]:.2f}")

# Step 4: Drop one feature from each highly correlated pair (optional)
features_to_drop = [pair[1] for pair in high_corr_pairs]  # Drop the second feature in each pair
df_reduced = df.drop(columns=features_to_drop)

print("\nDropped Features:")
print(features_to_drop)

# Step 5: Verify the reduced DataFrame
print("\nReduced DataFrame Columns:")
print(df_reduced.columns)

# Select only numeric columns
numeric_df = df.select_dtypes(include=['float64', 'int64'])

# Compute correlation matrix
correlation_matrix = numeric_df.corr()

# Display correlation matrix
print("Correlation Matrix:")
print(correlation_matrix)

# Step 2: Compute the correlation matrix for the entire DataFrame
correlation_matrix = df.corr()

# Step 3: Create an interactive heatmap for the correlation matrix
fig = go.Figure(data=go.Heatmap(
    z=correlation_matrix.values,  # Correlation values
    x=correlation_matrix.columns,  # Column names for x-axis
    y=correlation_matrix.columns,  # Column names for y-axis
    colorscale='Cividis',  # Color scheme (can be changed to 'Cividis', 'Blues', etc.)
    colorbar=dict(title="Correlation"),  # Add a color bar legend
))

# Update layout for better readability
fig.update_layout(
    title='Interactive Correlation Matrix (Entire Dataset)',
    xaxis=dict(tickangle=45),  # Rotate x-axis labels for better readability
    yaxis=dict(tickangle=0),
    autosize=True,
)

# Show the heatmap
fig.show()

# Compute correlation matrix
correlation_matrix = df.corr()

# Identify highly correlated feature pairs
threshold = 0.9
high_corr_pairs = []

for i in range(len(correlation_matrix.columns)):
    for j in range(i):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))

# Display highly correlated pairs
print("\nHighly Correlated Feature Pairs (|correlation| > 0.9):")
for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]} with correlation = {pair[2]:.2f}")

# Drop one feature from each pair
features_to_drop = [pair[1] for pair in high_corr_pairs]
df_reduced = df.drop(columns=features_to_drop)

print("\nDropped Features:")
print(features_to_drop)

df_reduced

from sklearn.decomposition import PCA

#  'scaled_data' contains the scaled dataset used for PCA
pca = PCA(n_components=5)  # Adjust number of components as needed
pca.fit(scaled_data)

# Print explained variance ratio for each component
print("Explained Variance Ratio for PCA Components:")
print(pca.explained_variance_ratio_)

"""This indicates:
The first principal component (PC1) explains nearly all the variance in the dataset (~99.9999%).
The remaining components contribute negligibly to the variance.
What this tells us:
The dataset's variance is heavily dominated by a single feature or combination of features.
PCA has not effectively reduced dimensionality because one component (PC1) is capturing almost all the information.
2. Correlation Between PCA Features and Original Features
From the correlation matrix:
pca_feature_1 and targtype1: Correlation = 1.00
pca_feature_1 and targsubtype1: Correlation = 0.92
pca_feature_2 and attacktype1: Correlation = 1.00
pca_feature_2 and attacktype1_txt: Correlation = 0.93
What this tells us:
pca_feature_1 is perfectly correlated with targtype1, meaning it is essentially a transformed version of this feature.
Similarly, pca_feature_2 is perfectly correlated with attacktype1.
These high correlations indicate that the PCA components are dominated by specific original features rather than combining information from multiple features.
Issues Identified
Redundancy in PCA Components:
Since pca_feature_1 and pca_feature_2 are highly correlated with specific original features, they do not provide additional information.
This redundancy suggests that PCA has not effectively combined features into new dimensions.
Ineffective Dimensionality Reduction:
The first principal component explains nearly all the variance, which means most of the other features contribute very little to the overall information.
Dominance of Specific Features:
Features like targtype1 and attacktype1 dominate the dataset's variance, overshadowing other features.
Recommended Actions
Action 1: Drop Redundant PCA Components
Since pca_feature_1 and pca_feature_2 are redundant (highly correlated with original features),can drop them from the dataset.
"""

# Drop redundant PCA components
df_reduced

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score
# Define features (X) and target (y) for classification and regression tasks
X = df_reduced.drop(columns=['success', 'nkill'])  # Features (drop target columns)
y_classification = df_reduced['success']  # Target for classification
y_regression = df_reduced['nkill']  # Target for regression

# Step 3: Train-Test Split
X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)
_, _, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)

# Step 4: Scale Numerical Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 5: Classification Models

# Logistic Regression
logistic_model = LogisticRegression()
logistic_model.fit(X_train_scaled, y_train_class)
logistic_preds = logistic_model.predict(X_test_scaled)
logistic_acc = accuracy_score(y_test_class, logistic_preds)
print(f"Logistic Regression Accuracy: {logistic_acc:.2f}")

# Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train_scaled, y_train_class)
rf_preds = rf_classifier.predict(X_test_scaled)
rf_acc = accuracy_score(y_test_class, rf_preds)
print(f"Random Forest Classifier Accuracy: {rf_acc:.2f}")

# Support Vector Machine (SVM)
svm_model = SVC()
svm_model.fit(X_train_scaled, y_train_class)
svm_preds = svm_model.predict(X_test_scaled)
svm_acc = accuracy_score(y_test_class, svm_preds)
print(f"SVM Accuracy: {svm_acc:.2f}")

# Step 6: Regression Models

# Linear Regression
linear_model = LinearRegression()
linear_model.fit(X_train_scaled, y_train_reg)
linear_preds = linear_model.predict(X_test_scaled)
linear_r2 = r2_score(y_test_reg, linear_preds)
print(f"Linear Regression R^2 Score: {linear_r2:.2f}")

# Random Forest Regressor
rf_regressor = RandomForestRegressor(random_state=42)
rf_regressor.fit(X_train_scaled, y_train_reg)
rf_reg_preds = rf_regressor.predict(X_test_scaled)
rf_reg_r2 = r2_score(y_test_reg, rf_reg_preds)
print(f"Random Forest Regressor R^2 Score: {rf_reg_r2:.2f}")

# Gradient Boosting Regressor
gb_regressor = GradientBoostingRegressor(random_state=42)
gb_regressor.fit(X_train_scaled, y_train_reg)
gb_reg_preds = gb_regressor.predict(X_test_scaled)
gb_reg_r2 = r2_score(y_test_reg, gb_reg_preds)
print(f"Gradient Boosting Regressor R^2 Score: {gb_reg_r2:.2f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
from sklearn.metrics import mean_squared_error, r2_score
# Confusion matrix for Logistic Regression
logistic_cm = confusion_matrix(y_test_class, logistic_preds)
print("Logistic Regression Classification Report:")
print(classification_report(y_test_class, logistic_preds))

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(logistic_cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Failure', 'Success'], yticklabels=['Failure', 'Success'])
plt.title("Confusion Matrix: Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve for Logistic Regression
logistic_probs = logistic_model.predict_proba(X_test_scaled)[:, 1]  # Probabilities for the positive class
fpr, tpr, _ = roc_curve(y_test_class, logistic_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"Logistic Regression (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.show()

# Evaluate Linear Regression
linear_mse = mean_squared_error(y_test_reg, linear_preds)
linear_r2 = r2_score(y_test_reg, linear_preds)
print(f"Linear Regression - MSE: {linear_mse:.2f}, R^2: {linear_r2:.2f}")

# Evaluate Random Forest Regressor
rf_reg_mse = mean_squared_error(y_test_reg, rf_reg_preds)
rf_reg_r2 = r2_score(y_test_reg, rf_reg_preds)
print(f"Random Forest Regressor - MSE: {rf_reg_mse:.2f}, R^2: {rf_reg_r2:.2f}")

# Evaluate Gradient Boosting Regressor
gb_reg_mse = mean_squared_error(y_test_reg, gb_reg_preds)
gb_reg_r2 = r2_score(y_test_reg, gb_reg_preds)
print(f"Gradient Boosting Regressor - MSE: {gb_reg_mse:.2f}, R^2: {gb_reg_r2:.2f}")

# Scatter plot for Linear Regression
plt.figure(figsize=(8, 6))
plt.scatter(y_test_reg, linear_preds, alpha=0.5)
plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'r--')  # Perfect prediction line
plt.title("Linear Regression: Predictions vs Actual Values")
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.show()

# Scatter plot for Linear Regression
fig = px.scatter(
    x=y_test_reg,
    y=linear_preds,
    labels={"x": "Actual Values", "y": "Predicted Values"},
    title="Linear Regression: Actual vs Predicted Values"
)
fig.add_shape(
    type="line",
    x0=y_test_reg.min(),
    y0=y_test_reg.min(),
    x1=y_test_reg.max(),
    y1=y_test_reg.max(),
    line=dict(color="Red", dash="dash"),
)  # Add a perfect prediction line
fig.show()

# Residuals for Linear Regression
residuals = y_test_reg - linear_preds

fig = px.scatter(
    x=y_test_reg,
    y=residuals,
    labels={"x": "Actual Values", "y": "Residuals"},
    title="Residual Plot: Linear Regression"
)
fig.add_hline(y=0, line_dash="dash", line_color="red")
fig.show()

# Metrics for Classification Models
classification_metrics = {
    "Model": ["Logistic Regression", "Random Forest", "SVM"],
    "Accuracy": [logistic_acc, rf_acc, svm_acc],
}

# Metrics for Regression Models
regression_metrics = {
    "Model": ["Linear Regression", "Random Forest Regressor", "Gradient Boosting Regressor"],
    "R^2 Score": [linear_r2, rf_reg_r2, gb_reg_r2],
    "MSE": [mean_squared_error(y_test_reg, linear_preds),
            mean_squared_error(y_test_reg, rf_reg_preds),
            mean_squared_error(y_test_reg, gb_reg_preds)],
}

# Display interactive tables using Plotly
classification_table = go.Figure(data=[go.Table(
    header=dict(values=list(classification_metrics.keys()), fill_color='paleturquoise', align='left'),
    cells=dict(values=list(classification_metrics.values()), fill_color='lavender', align='left')
)])
classification_table.update_layout(title_text="Classification Metrics")
classification_table.show()

regression_table = go.Figure(data=[go.Table(
    header=dict(values=list(regression_metrics.keys()), fill_color='paleturquoise', align='left'),
    cells=dict(values=list(regression_metrics.values()), fill_color='lavender', align='left')
)])
regression_table.update_layout(title_text="Regression Metrics")
regression_table.show()

# Random Forest Confusion Matrix
rf_cm = confusion_matrix(y_test_class, rf_preds)
fig = px.imshow(
    rf_cm,
    text_auto=True,
    color_continuous_scale='Greens',
    labels=dict(x="Predicted", y="Actual", color="Count"),
    title="Confusion Matrix: Random Forest"
)
fig.update_xaxes(side="top")
fig.show()

# SVM Confusion Matrix
svm_cm = confusion_matrix(y_test_class, svm_preds)
fig = px.imshow(
    svm_cm,
    text_auto=True,
    color_continuous_scale='Reds',
    labels=dict(x="Predicted", y="Actual", color="Count"),
    title="Confusion Matrix: SVM"
)
fig.update_xaxes(side="top")
fig.show()

from sklearn.metrics import confusion_matrix, classification_report
import plotly.express as px

# Confusion Matrix for Training Set
train_preds = logistic_model.predict(X_train_scaled)  # Predictions on training set
train_cm = confusion_matrix(y_train_class, train_preds)

# Interactive heatmap for training set confusion matrix
fig = px.imshow(
    train_cm,
    text_auto=True,
    color_continuous_scale='Blues',
    labels=dict(x="Predicted", y="Actual", color="Count"),
    title="Confusion Matrix: Logistic Regression (Training Set)"
)
fig.update_xaxes(side="top")
fig.show()

# Confusion Matrix for Test Set
test_preds = logistic_model.predict(X_test_scaled)  # Predictions on test set
test_cm = confusion_matrix(y_test_class, test_preds)

# Interactive heatmap for test set confusion matrix
fig = px.imshow(
    test_cm,
    text_auto=True,
    color_continuous_scale='Blues',
    labels=dict(x="Predicted", y="Actual", color="Count"),
    title="Confusion Matrix: Logistic Regression (Test Set)"
)
fig.update_xaxes(side="top")
fig.show()

import plotly.express as px
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
# Calculate metrics for Logistic Regression
logistic_acc = accuracy_score(y_test_class, logistic_preds)
logistic_precision = precision_score(y_test_class, logistic_preds)
logistic_recall = recall_score(y_test_class, logistic_preds)
logistic_f1 = f1_score(y_test_class, logistic_preds)

# Calculate metrics for Random Forest Classifier
rf_acc = accuracy_score(y_test_class, rf_preds)
rf_precision = precision_score(y_test_class, rf_preds)
rf_recall = recall_score(y_test_class, rf_preds)
rf_f1 = f1_score(y_test_class, rf_preds)

# Calculate metrics for SVM
svm_acc = accuracy_score(y_test_class, svm_preds)
svm_precision = precision_score(y_test_class, svm_preds)
svm_recall = recall_score(y_test_class, svm_preds)
svm_f1 = f1_score(y_test_class, svm_preds)

# Create a DataFrame to store the metrics
metrics_data = {
    "Model": ["Logistic Regression", "Random Forest", "SVM"],
    "Accuracy": [logistic_acc, rf_acc, svm_acc],
    "Precision": [logistic_precision, rf_precision, svm_precision],
    "Recall": [logistic_recall, rf_recall, svm_recall],
    "F1 Score": [logistic_f1, rf_f1, svm_f1],
}

metrics_df = pd.DataFrame(metrics_data)
print(metrics_df)

# Accuracy Plot
fig = px.line(
    metrics_df,
    x="Model",
    y="Accuracy",
    title="Accuracy Comparison of Classification Models",
    markers=True,
)
fig.show()

# Precision Plot
fig = px.line(
    metrics_df,
    x="Model",
    y="Precision",
    title="Precision Comparison of Classification Models",
    markers=True,
)
fig.show()

# Recall Plot
fig = px.line(
    metrics_df,
    x="Model",
    y="Recall",
    title="Recall Comparison of Classification Models",
    markers=True,
)
fig.show()

# F1 Score Plot
fig = px.line(
    metrics_df,
    x="Model",
    y="F1 Score",
    title="F1 Score Comparison of Classification Models",
    markers=True,
)
fig.show()

# Melt the DataFrame for easier plotting
metrics_melted = metrics_df.melt(id_vars="Model", var_name="Metric", value_name="Value")

# Create an interactive line graph
fig = px.line(
    metrics_melted,
    x="Metric",
    y="Value",
    color="Model",
    markers=True,
    title="Classification Model Performance Metrics",
    labels={"Metric": "Performance Metric", "Value": "Score"},
)
fig.update_layout(legend_title="Model")
fig.show()

# Combine actual and predicted values for all models into one DataFrame
classification_results = pd.DataFrame({
    "Actual": y_test_class,
    "Logistic Regression": logistic_preds,
    "Random Forest": rf_preds,
    "SVM": svm_preds,
})

# Melt the DataFrame for easier plotting
classification_results_melted = classification_results.melt(
    id_vars="Actual",
    var_name="Model",
    value_name="Predicted"
)

# Create a grouped bar chart to compare actual vs predicted
fig = px.bar(
    classification_results_melted,
    x="Model",
    color="Predicted",
    barmode="group",
    facet_col="Actual",
    title="Actual vs Predicted Comparison for Classification Models",
    labels={"Predicted": "Predicted Value", "Actual": "Actual Value"},
)
fig.show()

# Combine actual and predicted values for all models into one DataFrame
regression_results = pd.DataFrame({
    "Actual": y_test_reg,
    "Linear Regression": linear_preds,
    "Random Forest Regressor": rf_reg_preds,
    "Gradient Boosting Regressor": gb_reg_preds,
})

# Melt the DataFrame for easier plotting
regression_results_melted = regression_results.melt(
    id_vars="Actual",
    var_name="Model",
    value_name="Predicted"
)

# Create a scatter plot comparing actual vs predicted values for regression models
fig = px.scatter(
    regression_results_melted,
    x="Actual",
    y="Predicted",
    color="Model",
    title="Actual vs Predicted Values for Regression Models",
    labels={"Actual": "Actual Values", "Predicted": "Predicted Values"},
)

# Add a line of perfect prediction (y = x)
fig.add_shape(
    type="line",
    x0=regression_results["Actual"].min(),
    y0=regression_results["Actual"].min(),
    x1=regression_results["Actual"].max(),
    y1=regression_results["Actual"].max(),
    line=dict(color="Red", dash="dash"),
)

fig.show()

# Add counts of matches/mismatches for each model
classification_summary = classification_results_melted.groupby(["Model", "Actual", "Predicted"]).size().reset_index(name="Count")

# Create an interactive grouped bar chart
fig = px.bar(
    classification_summary,
    x="Model",
    y="Count",
    color="Predicted",
    facet_col="Actual",
    barmode="group",
    title="Classification: Actual vs Predicted Counts Across Models",
)
fig.show()

