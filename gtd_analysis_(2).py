# -*- coding: utf-8 -*-
"""GTD Analysis (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EBT8oE4CoRGI24dcEePvi1CMl2KNPIXq
"""



import pandas as pd
import numpy as np

file_path = 'globalterrorismdb_0522dist.xlsx'

# Read the CSV file using the 'latin-1' encoding
df = pd.read_excel('/content/drive/MyDrive/globalterrorismdb_0522dist.xlsx')

# Now we can work with the DataFrame 'df' containing the data from the CSV file
print(df.head())  # Print the first few rows of the DataFrame as a check

from google.colab import drive
drive.mount('/content/drive')

# Display dataset information
data_info = df.info()

data_info

# Display descriptive statistics
data_describe = df.describe()

data_describe

#The column eventid appears to be a unique identifier for each event.
#The dataset spans from the year 1970 (iyear) to 2017.
#Some columns like latitude and longitude have missing values, as the count is less than the total number of entries.
#Several columns, such as ransomamt, ransompaid, and nreleased, show a minimum value of -99. This might indicate a placeholder or missing value.
#Columns like INT_LOG, INT_IDEO, and INT_MISC have values ranging from -9 to 1, which suggests some categorical or ordinal encoding.

# Set the 'max_columns' option to None to print all columns
pd.set_option('display.max_columns', None)

# Now, when you print the DataFrame, it will display all columns without truncation
print(df)

# Calculate the number and percentage of missing values for each column
missing_data = pd.DataFrame(df.isnull().sum(), columns=['missing_count'])
missing_data['missing_percentage'] = (missing_data['missing_count'] / len(df)) * 100

# Sort the dataframe by percentage of missing values in descending order
missing_data = missing_data.sort_values(by='missing_percentage', ascending=False)

missing_data.head(20)

# Define a threshold for dropping columns
threshold = 80

# Identify columns to drop
columns_to_drop = missing_data[missing_data['missing_percentage'] > threshold].index

# Drop the identified columns
data_cleaned = df.drop(columns=columns_to_drop)

# Check the shape of the cleaned data
data_cleaned.shape

data_cleaned

# Compute the correlation matrix only for numeric columns
numeric_columns = data_cleaned.select_dtypes(include=['int64', 'float64'])
correlation_matrix_numeric = numeric_columns.corr()

correlation_matrix_numeric

# Check for missing values in data_cleaned
missing_data_cleaned = pd.DataFrame(data_cleaned.isnull().sum(), columns=['missing_count'])
missing_data_cleaned['missing_percentage'] = (missing_data_cleaned['missing_count'] / len(data_cleaned)) * 100

# Sort the dataframe by missing percentage in descending order
missing_data_cleaned = missing_data_cleaned.sort_values(by='missing_percentage', ascending=False)

missing_data_cleaned.head(20)

# Separate numerical and categorical columns
numerical_cols = data_cleaned.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = data_cleaned.select_dtypes(include=['object']).columns

# Impute missing values in numerical columns with median
for col in numerical_cols:
    data_cleaned[col].fillna(data_cleaned[col].median(), inplace=True)

# Impute missing values in categorical columns with mode
for col in categorical_cols:
    mode_value = data_cleaned[col].mode()[0]
    data_cleaned[col].fillna(mode_value, inplace=True)

# Verify if all missing values are filled
missing_after_imputation = data_cleaned.isnull().sum().sum()

missing_after_imputation

data_cleaned.isnull().sum()

import plotly.express as px

# Distribution of Attacks Over the Years
yearly_attacks = data_cleaned['iyear'].value_counts().sort_index()
fig_yearly_attacks = px.line(yearly_attacks, x=yearly_attacks.index, y=yearly_attacks.values, labels={'x':'Year', 'y':'Number of Attacks'})
fig_yearly_attacks.update_layout(title='Distribution of Attacks Over the Years')

fig_yearly_attacks

# 1. Number of Attacks Over the Years
attacks_by_year = data_cleaned['iyear'].value_counts().sort_index()
fig1 = px.line(attacks_by_year, x=attacks_by_year.index, y=attacks_by_year.values, labels={'x':'Year', 'y':'Number of Attacks'})
fig1.update_layout(title='Number of Terror Attacks Over the Years')
fig1.show()

# 2. Top Countries Affected by Terrorism
top_countries = data_cleaned['country_txt'].value_counts().head(10)
fig2 = px.bar(top_countries, x=top_countries.index, y=top_countries.values, labels={'x':'Country', 'y':'Number of Attacks'}, color_discrete_sequence=px.colors.qualitative.D3)
fig2.update_layout(title='Top 10 Countries Affected by Terrorism')
fig2.show()

# 3. Distribution of Attacks by Attack Type
attack_types = data_cleaned['attacktype1_txt'].value_counts()
fig3 = px.bar(attack_types, x=attack_types.index, y=attack_types.values, labels={'x':'Attack Type', 'y':'Number of Attacks'}, color_discrete_sequence=px.colors.qualitative.T10)
fig3.update_layout(title='Distribution of Attacks by Attack Type')
fig3.show()

# 4. Distribution of Attacks by Target Type
target_types = data_cleaned['targtype1_txt'].value_counts()
fig4 = px.bar(target_types, x=target_types.index, y=target_types.values, labels={'x':'Target Type', 'y':'Number of Attacks'}, color_discrete_sequence=px.colors.qualitative.Pastel)
fig4.update_layout(title='Distribution of Attacks by Target Type')
fig4.show()

# 5. Number of Fatalities Over the Years
fatalities_by_year = data_cleaned.groupby('iyear')['nkill'].sum()
fig5 = px.line(fatalities_by_year, x=fatalities_by_year.index, y=fatalities_by_year.values, labels={'x':'Year', 'y':'Number of Fatalities'})
fig5.update_layout(title='Number of Fatalities Due to Terror Attacks')

from sklearn.preprocessing import LabelEncoder

# Initialize LabelEncoder
le = LabelEncoder()

# Apply label encoding to each categorical column
for col in categorical_cols:
    data_cleaned[col] = le.fit_transform(data_cleaned[col])

# Check the data types after conversion
data_cleaned_dtypes_after_conversion = data_cleaned.dtypes

data_cleaned_dtypes_after_conversion

data_cleaned

data_cleaned['region_txt']

import plotly.figure_factory as ff

# Create the heatmap again using plotly, converting DataFrame index and columns to lists
fig = ff.create_annotated_heatmap(
    z=correlation_matrix_numeric.values,
    x=list(correlation_matrix_numeric.columns),
    y=list(correlation_matrix_numeric.columns),
    annotation_text=correlation_matrix_numeric.round(2).values,
    colorscale='Viridis'
)

# Update layout for better visualization
fig.update_layout(
    title='Correlation Matrix',
    xaxis=dict(title='Columns', tickangle=45),
    yaxis=dict(title='Columns'),
    width=1000,
    height=1000
)

fig.show()

# Sample a subset of the dataset (e.g., 10% of the data)
sample_data_corr = data_cleaned.sample(frac=0.1, random_state=42)

# Create a correlation matrix for the sampled data
corr_matrix_sample = sample_data_corr.corr().abs()

# Select upper triangle of correlation matrix for the sampled data
upper_triangle_sample = corr_matrix_sample.where(np.triu(np.ones(corr_matrix_sample.shape), k=1).astype(bool))

# Identify columns to drop based on threshold
columns_to_drop_sample = [column for column in upper_triangle_sample.columns if any(upper_triangle_sample[column] > threshold)]

# Drop correlated columns from the sampled data
data_reduced_sample = sample_data_corr.drop(columns_to_drop_sample, axis=1)

columns_to_drop_sample, data_reduced_sample.shape

# Import the StandardScaler and standardize the reduced sampled data
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
scaler_corrected = StandardScaler()
sample_data_standardized_corrected = scaler_corrected.fit_transform(data_reduced_sample)

# Apply PCA on the correctly standardized reduced sampled data
pca_sample_corrected = PCA()
pca_sample_data_corrected = pca_sample_corrected.fit_transform(sample_data_standardized_corrected)

# Calculate explained variance by each component for the correctly standardized reduced sampled data
explained_variance_ratio_sample_corrected = pca_sample_corrected.explained_variance_ratio_.cumsum()

# Plot the explained variance for the correctly standardized reduced sampled data
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio_sample_corrected) + 1), explained_variance_ratio_sample_corrected, marker='o', linestyle='--')
plt.title('Explained Variance by Components (Correctly Standardized Reduced Sampled Data)')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.grid(True)
plt.show()

explained_variance_ratio_sample_corrected

# Selecting a single feature for visualization: 'iday'
feature_example = data_reduced_sample['iday']

# Applying StandardScaler to the selected feature
feature_standardized = scaler_corrected.fit_transform(feature_example.values.reshape(-1, 1))

# Plotting original vs standardized feature
fig, ax = plt.subplots(1, 2, figsize=(15, 6))

# Original feature
ax[0].hist(feature_example, bins=30, color='blue', alpha=0.7)
ax[0].set_title('Original "iday" Feature Distribution')
ax[0].set_xlabel('iday')
ax[0].set_ylabel('Frequency')

# Standardized feature
ax[1].hist(feature_standardized, bins=30, color='green', alpha=0.7)
ax[1].set_title('Standardized "iday" Feature Distribution')
ax[1].set_xlabel('Standardized iday')
ax[1].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Replotting the explained variance for the correctly standardized reduced sampled data

plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio_sample_corrected) + 1), explained_variance_ratio_sample_corrected, marker='o', linestyle='--')
plt.title('Explained Variance by Components (Correctly Standardized Reduced Sampled Data)')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')
plt.axhline(y=0.95, color='r', linestyle='-')  # 95% explained variance line
plt.grid(True)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Extracting the target variable 'nkill' and features from the sampled data
X = data_reduced_sample.drop(['nkill'], axis=1)
y = data_reduced_sample['nkill']

# Splitting the data into training and test sets (70% train, 30% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardizing the training and test data
X_train_standardized = scaler_corrected.fit_transform(X_train)
X_test_standardized = scaler_corrected.transform(X_test)

# Checking the shape of the training and test sets
X_train_standardized.shape, X_test_standardized.shape

# Training a linear regression model
linear_reg_model = LinearRegression()
linear_reg_model.fit(X_train_standardized, y_train)

# Predicting on the test data
y_pred = linear_reg_model.predict(X_test_standardized)

# Calculating performance metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)  # squared=False gives RMSE

mae, rmse

#Mean Absolute Error (MAE): 2.22
#On average, the model's predictions are about 2.19 kills away from the actual number of kills.
#Root Mean Squared Error (RMSE): 7.05
#This metric provides information about the magnitude of error between predicted and actual values, with a higher weight to large errors.

from sklearn.linear_model import Ridge, Lasso

# Initialize and train the Ridge regression model
ridge_model = Ridge(alpha=1.0)  # alpha is the regularization strength
ridge_model.fit(X_train_standardized, y_train)

# Predicting on the test data
y_pred_ridge = ridge_model.predict(X_test_standardized)

# Calculating performance metrics for the Ridge regression model
mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)

mae_ridge, rmse_ridge

#The metrics are quite similar to the baseline linear regression model. This suggests that the linear regression model was not overfitting much, or the regularization strength (alpha) we used for Ridge regression might not be optimal.

# Initialize and train the Lasso regression model
lasso_model = Lasso(alpha=0.01)  # alpha is the regularization strength, using a small value to prevent excessive coefficient shrinkage
lasso_model.fit(X_train_standardized, y_train)

# Predicting on the test data
y_pred_lasso = lasso_model.predict(X_test_standardized)

# Calculating performance metrics for the Lasso regression model
mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
rmse_lasso = mean_squared_error(y_test, y_pred_lasso, squared=False)

mae_lasso, rmse_lasso

import plotly.graph_objects as go

# Model names
models = ['Linear Regression', 'Ridge Regression', 'Lasso Regression']

# MAE and RMSE values for each model
mae_values = [mae, mae_ridge, mae_lasso]
rmse_values = [rmse, rmse_ridge, rmse_lasso]

# Creating interactive bar plots
fig = go.Figure()

# Adding MAE bars
fig.add_trace(go.Bar(x=models, y=mae_values, name='MAE', marker_color='blue'))

# Adding RMSE bars
fig.add_trace(go.Bar(x=models, y=rmse_values, name='RMSE', marker_color='green'))

# Layout settings
fig.update_layout(title='Comparison of Regression Models Performance',
                  xaxis_title='Model',
                  yaxis_title='Error Value',
                  barmode='group')

fig.show()

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

# Splitting the data
X_wound = data_reduced_sample.drop(['nwound'], axis=1)
y_wound = data_reduced_sample['nwound']
X_train_wound, X_test_wound, y_train_wound, y_test_wound = train_test_split(X_wound, y_wound, test_size=0.3, random_state=42)

# Standardizing the data
scaler = StandardScaler()
X_train_wound_standardized = scaler.fit_transform(X_train_wound)
X_test_wound_standardized = scaler.transform(X_test_wound)

# Hyperparameter tuning using GridSearchCV
param_grid = {
    'max_depth': [None, 10, 20, 30, 40],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt']
}
grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)
grid_search.fit(X_train_wound_standardized, y_train_wound)
best_params = grid_search.best_params_

# Training the Decision Tree Regressor with the best hyperparameters
dt_regressor_tuned = DecisionTreeRegressor(**best_params, random_state=42)
dt_regressor_tuned.fit(X_train_wound_standardized, y_train_wound)

# Predicting and evaluating
y_pred_dt_tuned = dt_regressor_tuned.predict(X_test_wound_standardized)
mae_dt_tuned = mean_absolute_error(y_test_wound, y_pred_dt_tuned)
rmse_dt_tuned = mean_squared_error(y_test_wound, y_pred_dt_tuned, squared=False)

mae_dt_tuned, rmse_dt_tuned

from sklearn.ensemble import GradientBoostingRegressor

# Initializing and training the Gradient Boosting Regressor
gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
gb_regressor.fit(X_train_wound_standardized, y_train_wound)

# Predicting on the test data with the Gradient Boosting Regressor
y_pred_gb = gb_regressor.predict(X_test_wound_standardized)

# Calculating performance metrics for the Gradient Boosting Regressor
mae_gb = mean_absolute_error(y_test_wound, y_pred_gb)
rmse_gb = mean_squared_error(y_test_wound, y_pred_gb, squared=False)

mae_gb, rmse_gb

import plotly.graph_objects as go

# Create interactive scatter plots for both models

# Plot for Decision Tree Regressor
fig_dt = go.Figure()
fig_dt.add_trace(go.Scatter(x=y_test_wound, y=y_pred_dt_tuned,
                    mode='markers',
                    name='Decision Tree Predictions',
                    marker=dict(color='blue')))
fig_dt.add_trace(go.Scatter(x=[0, max(y_test_wound)], y=[0, max(y_test_wound)],
                    mode='lines',
                    name='Perfect Predictions',
                    line=dict(color='red')))
fig_dt.update_layout(title='Decision Tree Regressor: Actual vs. Predicted',
                     xaxis_title='Actual nwound',
                     yaxis_title='Predicted nwound',
                     showlegend=True)

# Plot for Gradient Boosting Regressor
fig_gb = go.Figure()
fig_gb.add_trace(go.Scatter(x=y_test_wound, y=y_pred_gb,
                    mode='markers',
                    name='Gradient Boosting Predictions',
                    marker=dict(color='green')))
fig_gb.add_trace(go.Scatter(x=[0, max(y_test_wound)], y=[0, max(y_test_wound)],
                    mode='lines',
                    name='Perfect Predictions',
                    line=dict(color='red')))
fig_gb.update_layout(title='Gradient Boosting Regressor: Actual vs. Predicted',
                     xaxis_title='Actual nwound',
                     yaxis_title='Predicted nwound',
                     showlegend=True)

fig_dt.show()
fig_gb.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Preparing data for classification task: Predicting success of a terror event
X_success = data_reduced_sample.drop(['success'], axis=1)
y_success = data_reduced_sample['success']

# Splitting the data into training and test sets
X_train_success, X_test_success, y_train_success, y_test_success = train_test_split(X_success, y_success, test_size=0.3, random_state=42)

# Standardizing the data
scaler_success = StandardScaler()
X_train_success_standardized = scaler_success.fit_transform(X_train_success)
X_test_success_standardized = scaler_success.transform(X_test_success)

# Training a logistic regression model for success prediction
logreg_model = LogisticRegression(random_state=42)
logreg_model.fit(X_train_success_standardized, y_train_success)

# Predicting on the test data
y_pred_success = logreg_model.predict(X_test_success_standardized)

# Evaluating the logistic regression model
accuracy_logreg = accuracy_score(y_test_success, y_pred_success)
classification_rep_logreg = classification_report(y_test_success, y_pred_success)

accuracy_logreg, classification_rep_logreg

# Preparing data for classification task: Predicting type of attack
X_attacktype = data_reduced_sample.drop(['attacktype1'], axis=1)
y_attacktype = data_reduced_sample['attacktype1']

# Splitting the data into training and test sets
X_train_attacktype, X_test_attacktype, y_train_attacktype, y_test_attacktype = train_test_split(X_attacktype, y_attacktype, test_size=0.3, random_state=42)

# Standardizing the data
X_train_attacktype_standardized = scaler_success.fit_transform(X_train_attacktype)
X_test_attacktype_standardized = scaler_success.transform(X_test_attacktype)

# Training a logistic regression model for attack type prediction
logreg_attacktype_model = LogisticRegression(random_state=42, max_iter=1000)
logreg_attacktype_model.fit(X_train_attacktype_standardized, y_train_attacktype)

# Predicting on the test data
y_pred_attacktype = logreg_attacktype_model.predict(X_test_attacktype_standardized)

# Evaluating the logistic regression model
accuracy_logreg_attacktype = accuracy_score(y_test_attacktype, y_pred_attacktype)
classification_rep_logreg_attacktype = classification_report(y_test_attacktype, y_pred_attacktype)

accuracy_logreg_attacktype, classification_rep_logreg_attacktype

# Preparing data for classification task: Predicting if an event involved suicide attacks
X_suicide = data_reduced_sample.drop(['suicide'], axis=1)
y_suicide = data_reduced_sample['suicide']

# Splitting the data into training and test sets
X_train_suicide, X_test_suicide, y_train_suicide, y_test_suicide = train_test_split(X_suicide, y_suicide, test_size=0.3, random_state=42)

# Standardizing the data
X_train_suicide_standardized = scaler_success.fit_transform(X_train_suicide)
X_test_suicide_standardized = scaler_success.transform(X_test_suicide)

# Training a logistic regression model for suicide prediction
logreg_suicide_model = LogisticRegression(random_state=42)
logreg_suicide_model.fit(X_train_suicide_standardized, y_train_suicide)

# Predicting on the test data
y_pred_suicide = logreg_suicide_model.predict(X_test_suicide_standardized)

# Evaluating the logistic regression model
accuracy_logreg_suicide = accuracy_score(y_test_suicide, y_pred_suicide)
classification_rep_logreg_suicide = classification_report(y_test_suicide, y_pred_suicide)

accuracy_logreg_suicide, classification_rep_logreg_suicide

import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix
def plot_interactive_confusion_matrix(cm, labels, title):
    """
    Create an interactive confusion matrix using Plotly.
    """
    # Create heatmap
    fig = go.Figure(data=go.Heatmap(
                    z=cm,
                    x=labels,
                    y=labels,
                    colorscale='Viridis',
                    showscale=True))

    # Update layout
    fig.update_layout(title=title,
                      xaxis=dict(title='Predicted Label', side='bottom'),
                      yaxis=dict(title='True Label', autorange="reversed"))
    return fig

# Get unique labels for attacktype1 for plotting
unique_attack_labels = sorted(y_test_attacktype.unique())

# Generate confusion matrices for each classification task
cm_success = confusion_matrix(y_test_success, y_pred_success)
cm_attacktype = confusion_matrix(y_test_attacktype, y_pred_attacktype, labels=unique_attack_labels)
cm_suicide = confusion_matrix(y_test_suicide, y_pred_suicide)

# Plotting
fig_success = plot_interactive_confusion_matrix(cm_success, ["Unsuccessful", "Successful"], 'Confusion Matrix: Prediction of Success')
fig_attacktype = plot_interactive_confusion_matrix(cm_attacktype, unique_attack_labels, 'Confusion Matrix: Prediction of Attack Type')
fig_suicide = plot_interactive_confusion_matrix(cm_suicide, ["Non-suicide", "Suicide"], 'Confusion Matrix: Prediction of Suicide Attacks')

fig_success.show()
fig_attacktype.show()
fig_suicide.show()

# For 'success' classification task
fig_success_distribution = px.histogram(data_reduced_sample, x='success', title='Distribution of Success Classes')
fig_success_distribution.show()

# For 'attacktype1' classification task
fig_attacktype_distribution = px.histogram(data_reduced_sample, x='attacktype1', title='Distribution of Attack Types')
fig_attacktype_distribution.show()

# For 'suicide' classification task
fig_suicide_distribution = px.histogram(data_reduced_sample, x='suicide', title='Distribution of Suicide Classes')
fig_suicide_distribution.show()

# Extract coefficients from the logistic regression model
coefficients = logreg_attacktype_model.coef_[0]
features = X_train_attacktype.columns

# Create a DataFrame for coefficients
df_coefficients = pd.DataFrame({
    'Feature': features,
    'Coefficient': coefficients
}).sort_values(by='Coefficient', ascending=False)

# Plotting
fig_coefficients = px.bar(df_coefficients, x='Feature', y='Coefficient', title='Logistic Regression Coefficients')
fig_coefficients.show()

import pandas as pd
import plotly.express as px

# Use the latitude and longitude columns to create a scatter plot on a world map
fig = px.scatter_geo(data_cleaned,
                     lat='latitude',
                     lon='longitude',
                     title='Locations of Terrorist Incidents Worldwide',
                     template='plotly',
                     projection='orthographic',
                     opacity=0.5,
                     hover_name='eventid',  # or any other column you'd like to display on hover
                     hover_data=['country_txt', 'city', 'iyear', 'attacktype1_txt'],  # additional columns to show on hover
                     color='region_txt'  # color dots by region
                    )

fig.update_geos(showland=True, landcolor='white', showocean=True, oceancolor='lightblue')
fig.show()













